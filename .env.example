# --- SECURITY TEMPLATE ---
# Copy this file to '.env' and fill in secrets locally.

# Hugging Face (Required for Model Inference)
HF_TOKEN=

# Google Gemini (Optional)
GOOGLE_API_KEY=

# Heady Services (Required for MCP Integration)
HEADY_API_KEY=

# Optional: GitHub Enterprise Token
GH_TOKEN=

# --- Server Configuration ---
PORT=3300
NODE_ENV=development

# --- Model Configuration ---
HF_TEXT_MODEL=gpt2
HF_EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2

# --- Rate Limiting ---
HEADY_RATE_LIMIT_WINDOW_MS=60000
HEADY_RATE_LIMIT_MAX=120

# --- Concurrency Limits ---
HF_MAX_CONCURRENCY=4
HEADY_PY_MAX_CONCURRENCY=2

# --- Python Worker Configuration ---
HEADY_PYTHON_BIN=python
HEADY_PY_WORKER_TIMEOUT_MS=90000
HEADY_HF_TIMEOUT_S=60
HEADY_HF_MAX_RETRIES=2
HEADY_HF_USER_AGENT=heady-python-worker/1.0

# --- QA Configuration ---
HEADY_QA_BACKEND=auto
HEADY_QA_MAX_NEW_TOKENS=256
HEADY_QA_MAX_QUESTION_CHARS=4000
HEADY_QA_MAX_CONTEXT_CHARS=12000
HEADY_QA_TRUNCATE_INPUTS=true

# --- Admin UI Configuration ---
HEADY_ADMIN_ROOT=
HEADY_ADMIN_ALLOWED_PATHS=
HEADY_ADMIN_MAX_BYTES=512000
HEADY_ADMIN_OP_LOG_LIMIT=2000
HEADY_ADMIN_OP_LIMIT=50

# --- GPU Configuration ---
HEADY_ADMIN_ENABLE_GPU=false
REMOTE_GPU_HOST=
REMOTE_GPU_PORT=
GPU_MEMORY_LIMIT=
ENABLE_GPUDIRECT=false

# --- CORS Configuration ---
HEADY_CORS_ORIGINS=http://localhost:3000,http://localhost:3300

# --- Proxy Configuration ---
HEADY_TRUST_PROXY=false

# --- MCP Configuration ---
HEADY_MCP_SERVER_TIMEOUT_MS=30000
HEADY_MCP_MAX_CONCURRENT_REQUESTS=10

# --- Logging Configuration ---
HEADY_LOG_LEVEL=info
HEADY_LOG_FORMAT=json

# --- Security Configuration ---
HEADY_SESSION_SECRET=
HEADY_JWT_EXPIRES_IN=24h
